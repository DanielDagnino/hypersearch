## Currently Supported Ops

- [x] Regularization

#### Initialization

Changing initialization for every layer

#### Regularization

- L1 decay
- L2 decay

- Kernel Regularizer
- Bias Regualizer

- FC
- Conv

#### Dropout

- Adding Dropout Layers
- Varying probability of Dropout Layers

#### Activation Function

Change activation function for fc or conv layers

#### Constraints

- Max Norm
- Non-Negativity

- FC
- Conv

#### Normalization

- Adding Batch Norm Layers

#### Optimization

- changing optimizer
- learning rate scheduling
- learning rate on plateau

#### Hidden Units

- Change # of hidden units in fc layers